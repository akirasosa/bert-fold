note: 'Faster, but lr scheduler does not work correctly. '
seed: 1

module_params:
  lr: 1e-5
  weight_decay: 0.
#  gradient_checkpointing: true

trainer_params:
  epochs: 25
  gpus: -1
  # Save weight on SSD
  weights_save_path: /mnt/ssdstuff/akirasosa/experiments/
  # It makes loss nan at first a few steps, as a result lr scheduler will be broken.
  amp_level: 'O2'
  accumulate_grad_batches: 2

data_params:
  batch_size: 6
